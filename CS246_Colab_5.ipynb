{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Investigator13th/CS246/blob/main/CS246_Colab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# CS246 - Colab 5\n",
        "## PageRank"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "print(\"Colab 5 Mascot\")\n",
        "Image(url='https://media.giphy.com/media/cCOVfFwDI3awdse5A3/giphy.gif',width=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "yUHXnec92vdy",
        "outputId": "30bd2e67-da38-49ac-e498-9325a67a9dd8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab 5 Mascot\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://media.giphy.com/media/cCOVfFwDI3awdse5A3/giphy.gif\" width=\"150\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUUjUvXe3Sjk"
      },
      "source": [
        "First of all, we authenticate a Google Drive client to download the dataset we will be processing in this Colab.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRElWs_x2mGh"
      },
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHsFTGUy2n1c"
      },
      "source": [
        "id='1EoolSK32_U74I4FeLox88iuUB_SUUYsI'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('web-Stanford.txt')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "If you executed the cells above, you should be able to see the dataset we will use for this Colab under the \"Files\" tab on the left panel.\n",
        "\n",
        "Next, we import some of the common libraries needed for our task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXzc_R6ArXtL"
      },
      "source": [
        "For this Colab we will be using [NetworkX](https://networkx.github.io), a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks ([tutorial](https://networkx.org/documentation/stable/tutorial.html)).\n",
        "\n",
        "The dataset we will analyze is a snapshot of the Web Graph centered around [stanford.edu](https://stanford.edu), collected in 2002. Nodes represent pages from Stanford University (stanford.edu) and directed edges represent hyperlinks between them. [[More Info]](http://snap.stanford.edu/data/web-Stanford.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPIadGxvLyyq"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.read_edgelist('web-Stanford.txt', create_using=nx.DiGraph)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to print graph information"
      ],
      "metadata": {
        "id": "zgVOrXN1EWNh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smd1XvR7MLyE"
      },
      "source": [
        "def print_graph_info(G, directed=True):\n",
        "  print(\"Number of nodes:\", len(G.nodes))\n",
        "  print(\"Number of edges:\", len(G.edges))\n",
        "  if directed:\n",
        "    print(\"Average in-degree:\", sum(dict(G.in_degree).values()) / len(G.nodes))\n",
        "    print(\"Average out-degree:\", sum(dict(G.out_degree).values()) / len(G.nodes))\n",
        "  else:\n",
        "    print(\"Average degree:\", sum(dict(G.degree).values()) / len(G.nodes))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_graph_info(G, True)"
      ],
      "metadata": {
        "id": "gcZxbivxEcDT",
        "outputId": "aa921270-c7ae-4048-c969-81852d463abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 281903\n",
            "Number of edges: 2312497\n",
            "Average in-degree: 8.203165627893283\n",
            "Average out-degree: 8.203165627893283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbmr23B2rJKR"
      },
      "source": [
        "### Your Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15OQeyys1xd"
      },
      "source": [
        "To begin with, let's simplify our analysis by ignoring the dangling nodes and the disconnected components in the original graph.\n",
        "\n",
        "Use NetworkX to identify the **largest** weakly connected component in the ```G``` graph.  **From now on, use this connected component for all the following tasks.**\n",
        "\n",
        "Print its information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9tDwRidIw-Q",
        "outputId": "88de6e03-0a4a-4513-e255-7e8b20e7cb3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' 3 lines of code in total expected. Execution time should be less than 30 seconds'''\n",
        "components = nx.weakly_connected_components(G)\n",
        "largest_component = max(components, key=len)\n",
        "largest_component_G = G.subgraph(largest_component)\n",
        "print_graph_info(largest_component_G, True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 255265\n",
            "Number of edges: 2234572\n",
            "Average in-degree: 8.753930229369479\n",
            "Average out-degree: 8.753930229369479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbYMNjBhuhK-"
      },
      "source": [
        "Compute the PageRank vector, using the default parameters in NetworkX: [https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html#networkx.algorithms.link_analysis.pagerank_alg.pageranky](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html#networkx.algorithms.link_analysis.pagerank_alg.pagerank)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll-rVh7KVoLA"
      },
      "source": [
        "''' 1 line of code in total expected. Execution time should be around or less than 15 seconds.'''\n",
        "\n",
        "# YOUR CODE HERE\n",
        "pagerank_vector = nx.pagerank(largest_component_G)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDx905Wk3FKf"
      },
      "source": [
        "In 1999, Barabási and Albert proposed an elegant mathematical model which can generate graphs with topological properties similar to the Web Graph (also called Scale-free Networks).\n",
        "\n",
        "If you complete the steps below, you should obtain some empirical evidence that the Random Graph model is inferior compared to the Barabási–Albert model when it comes to generating a graph resembling the World Wide Web!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox3ksWEFyaP-"
      },
      "source": [
        "As such, we will use two different graph generator methods, and then we will test how well they approximate the Web Graph structure by means of comparing the respective PageRank vectors. [[NetworkX Graph generators]](https://networkx.github.io/documentation/stable/reference/generators.html#)\n",
        "\n",
        "Using the parameters ```seed = 1``` and ```directed=False``` where applicable, generate:\n",
        "\n",
        "\n",
        "1.   a random graph (with the fast method), setting ```n``` equal to the number of nodes in the original connected component, and ```p = 0.00008```\n",
        "2.   a Barabasi-Albert graph (with the standard method), setting ```n``` equal to the number of nodes in the original connected component, and finding the right ***integer*** value for ```m``` such as the resulting number of edges **approximates by excess(the value that is closest to and satisfies $\\geq$)** the number of edges in the original connected component (the largest weakly connected component you attained above)\n",
        "\n",
        "and compute the PageRank vectors for both graphs. Print generated graph's information, if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yd94CE9aPJP"
      },
      "source": [
        "''' 6-8 lines of code in total expected but can differ based on your style.\n",
        "For sub-parts of the question (if any), creating different cells of code would be recommended.\n",
        "The execution time for one pagerank computation should be around 20 seconds.'''\n",
        "\n",
        "# YOUR CODE HERE\n",
        "import networkx as nx\n",
        "import math\n",
        "n = largest_component_G.number_of_nodes()\n",
        "m_edges = largest_component_G.number_of_edges()\n",
        "\n",
        "seed = 1\n",
        "p = 0.00008\n",
        "random_graph = nx.fast_gnp_random_graph(n=n, p=p, seed=seed, directed=False)\n",
        "\n",
        "# 生成 Barabási-Albert 图\n",
        "# 首先，计算 m 的值，使得生成的边数略大于原始图的边数\n",
        "m = int(math.ceil(m_edges / n))\n",
        "barabasi_albert_graph = nx.barabasi_albert_graph(n=n, m=m, seed=seed)\n",
        "\n",
        "# 计算两个新图的 PageRank 向量\n",
        "random_pagerank = nx.pagerank(random_graph)\n",
        "barabasi_pagerank = nx.pagerank(barabasi_albert_graph)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlxK42Pi01vN"
      },
      "source": [
        "Compare the PageRank vectors obtained on the generated graphs with the PageRank vector you computed on the original connected component (the largest weakly connected component you attained above).\n",
        "**Sort** the components of each vector by value, and use cosine similarity as similarity measure.\n",
        "\n",
        "Feel free to use any implementation of the cosine similarity available in third-party libraries, or implement your own with ```numpy```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aUgyeNdUQxs"
      },
      "source": [
        "''' 8-10 lines of code in total expected but can differ based on your style.\n",
        "For sub-parts of the question (if any), creating different cells of code would be recommended.'''\n",
        "\n",
        "# YOUR CODE HERE\n",
        "from scipy.spatial.distance import cosine\n",
        "def pagerank_to_sort_vector(pagerank_dict):\n",
        "    # sorted_items = sorted(pagerank_dict.items())\n",
        "    vector = np.array([item[1] for item in pagerank_dict.items()])\n",
        "    sorted_vector = np.sort(vector)\n",
        "    return sorted_vector\n",
        "    # return np.array([item[1] for item in sorted_items])\n",
        "\n",
        "def cosine_similarity(vec1, vec2):  ## cosine计算的是余弦距离\n",
        "    return 1 - cosine(vec1, vec2)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_pagerank_vector = pagerank_to_sort_vector(pagerank_vector)\n",
        "random_pagerank_vector = pagerank_to_sort_vector(random_pagerank)\n",
        "barabasi_pagerank_vector = pagerank_to_sort_vector(barabasi_pagerank)\n",
        "\n",
        "similarity_original_random = cosine_similarity(original_pagerank_vector, random_pagerank_vector)\n",
        "\n",
        "similarity_original_barabasi = cosine_similarity(original_pagerank_vector, barabasi_pagerank_vector)\n",
        "\n",
        "# 输出相似性\n",
        "print(f\"Similarity between original and random graph: {similarity_original_random}\")\n",
        "print(f\"Similarity between original and Barabási-Albert graph: {similarity_original_barabasi}\")\n"
      ],
      "metadata": {
        "id": "8jsF3pAwl7xn",
        "outputId": "9ae551b9-6980-4603-a5f8-dbaffb289ff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between original and random graph: 0.103955647039644\n",
            "Similarity between original and Barabási-Albert graph: 0.6488673660967236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrXJyVNP2AI"
      },
      "source": [
        "Once you have working code for each cell above, **head over to Gradescope, read carefully the questions, and submit your solution for this Colab**!"
      ]
    }
  ]
}